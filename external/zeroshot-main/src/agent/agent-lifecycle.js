// @ts-nocheck
/**
 * AgentLifecycle - Agent state machine and lifecycle management
 *
 * Provides:
 * - Agent startup and shutdown
 * - Message handling and routing
 * - Trigger action execution (execute_task, stop_cluster)
 * - Task execution with retry logic
 * - Liveness monitoring with multi-indicator stuck detection
 *
 * State machine: idle â†’ evaluating â†’ building_context â†’ executing â†’ idle
 */

const { buildContext } = require('./agent-context-builder');
const { findMatchingTrigger, evaluateTrigger } = require('./agent-trigger-evaluator');
const { executeHook } = require('./agent-hook-executor');
const {
  analyzeProcessHealth,
  isPlatformSupported,
  STUCK_THRESHOLD,
} = require('./agent-stuck-detector');

/**
 * Start the agent (begin listening for triggers)
 * @param {AgentWrapper} agent - Agent instance
 */
function start(agent) {
  if (agent.running) {
    throw new Error(`Agent ${agent.id} is already running`);
  }

  agent.running = true;
  agent.state = 'idle';

  // Subscribe to all messages for this cluster
  agent.unsubscribe = agent.messageBus.subscribe((message) => {
    if (message.cluster_id === agent.cluster.id) {
      handleMessage(agent, message).catch((error) => {
        // FATAL: Message handling failed - crash loud
        console.error(`\n${'='.repeat(80)}`);
        console.error(`ðŸ”´ FATAL: Agent ${agent.id} message handler crashed`);
        console.error(`${'='.repeat(80)}`);
        console.error(`Topic: ${message.topic}`);
        console.error(`Error: ${error.message}`);
        console.error(`Stack: ${error.stack}`);
        console.error(`${'='.repeat(80)}\n`);
        // Re-throw to crash the process - DO NOT SILENTLY CONTINUE
        throw error;
      });
    }
  });

  agent._log(`Agent ${agent.id} started (role: ${agent.role})`);
  agent._publishLifecycle('STARTED', {
    triggers: agent.config.triggers?.map((t) => t.topic) || [],
  });
}

/**
 * Stop the agent
 * Waits for any in-flight execution to complete before returning.
 * @param {AgentWrapper} agent - Agent instance
 * @returns {Promise<void>}
 */
async function stop(agent) {
  if (!agent.running) {
    return;
  }

  agent.running = false;
  agent.state = 'stopped';

  if (agent.unsubscribe) {
    agent.unsubscribe();
    agent.unsubscribe = null;
  }

  // Kill current task if any
  if (agent.currentTask) {
    agent._killTask();
  }

  // Wait for in-flight execution to complete (up to 5 seconds)
  // This prevents write-after-close race conditions
  if (agent._currentExecution) {
    try {
      await Promise.race([
        agent._currentExecution,
        new Promise((resolve) => setTimeout(resolve, 5000)),
      ]);
    } catch {
      // Ignore errors from cancelled execution
    }
    agent._currentExecution = null;
  }

  agent._log(`Agent ${agent.id} stopped`);
}

/**
 * Handle incoming message
 * @param {AgentWrapper} agent - Agent instance
 * @param {Object} message - Incoming message
 */
async function handleMessage(agent, message) {
  // Check if any trigger matches FIRST (before state check)
  const matchingTrigger = findMatchingTrigger({
    triggers: agent.config.triggers,
    message,
  });

  if (!matchingTrigger) {
    return; // No trigger for this message type
  }

  // Now check state - LOG if we're dropping a message we SHOULD handle
  if (!agent.running) {
    console.warn(`[${agent.id}] âš ï¸ DROPPING message (not running): ${message.topic}`);
    return;
  }
  if (agent.state !== 'idle') {
    console.warn(
      `[${agent.id}] âš ï¸ DROPPING message (busy, state=${agent.state}): ${message.topic}`
    );
    return;
  }

  // Evaluate trigger logic
  agent.state = 'evaluating_logic';

  const agentContext = {
    id: agent.id,
    role: agent.role,
    iteration: agent.iteration,
    cluster_id: agent.cluster.id,
  };

  const shouldExecute = evaluateTrigger({
    trigger: matchingTrigger,
    message,
    agent: agentContext,
    logicEngine: agent.logicEngine,
  });

  if (!shouldExecute) {
    agent.state = 'idle';
    return;
  }

  // Execute trigger action (lifecycle event published inside for execute_task)
  // Track execution so stop() can wait for it
  const executionPromise = executeTriggerAction(agent, matchingTrigger, message);
  agent._currentExecution = executionPromise;
  try {
    await executionPromise;
  } finally {
    // Clear only if this is still our execution (not replaced by another)
    if (agent._currentExecution === executionPromise) {
      agent._currentExecution = null;
    }
  }
}

/**
 * Execute trigger action
 * @param {AgentWrapper} agent - Agent instance
 * @param {Object} trigger - Matched trigger config
 * @param {Object} message - Triggering message
 */
async function executeTriggerAction(agent, trigger, message) {
  const action = trigger.action || 'execute_task';

  if (action === 'execute_task') {
    await executeTask(agent, message);
  } else if (action === 'stop_cluster') {
    // Publish CLUSTER_COMPLETE message to signal successful completion
    agent._publish({
      topic: 'CLUSTER_COMPLETE',
      receiver: 'system',
      content: {
        text: 'All validation passed. Cluster completing successfully.',
        data: {
          reason: 'all_validators_approved',
          timestamp: Date.now(),
        },
      },
    });
    agent.state = 'completed';
    agent._log(`Agent ${agent.id}: Cluster completion triggered`);
  } else {
    console.warn(`Unknown action: ${action}`);
    agent.state = 'idle';
  }
}

async function runOnStartHook({ agent, triggeringMessage }) {
  await executeHook({
    hook: agent.config.hooks?.onStart,
    agent: agent,
    message: triggeringMessage,
    result: undefined,
    messageBus: agent.messageBus,
    cluster: agent.cluster,
    orchestrator: agent.orchestrator,
  });
}

function hasReachedMaxIterations(agent) {
  return agent.iteration >= agent.maxIterations;
}

function handleMaxIterationsReached(agent) {
  agent._log(`[Agent ${agent.id}] Hit max iterations (${agent.maxIterations}), stopping cluster`);
  agent._publishLifecycle('MAX_ITERATIONS_REACHED', {
    iteration: agent.iteration,
    maxIterations: agent.maxIterations,
  });
  // Publish failure message - orchestrator watches for this and auto-stops
  agent._publish({
    topic: 'CLUSTER_FAILED',
    receiver: 'system',
    content: {
      text: `Agent ${agent.id} hit max iterations limit (${agent.maxIterations}). Stopping cluster.`,
      data: {
        reason: 'max_iterations',
        iteration: agent.iteration,
        maxIterations: agent.maxIterations,
      },
    },
  });
  agent.state = 'failed';
}

function buildTaskContext({ agent, triggeringMessage }) {
  agent.state = 'building_context';
  return buildContext({
    id: agent.id,
    role: agent.role,
    iteration: agent.iteration,
    config: agent.config,
    messageBus: agent.messageBus,
    cluster: agent.cluster,
    lastTaskEndTime: agent.lastTaskEndTime,
    triggeringMessage,
    selectedPrompt: agent._selectPrompt(),
  });
}

function logInputContext(agent, context) {
  if (agent.quiet) {
    return;
  }

  console.log(`\n${'='.repeat(80)}`);
  console.log(`ðŸ“¥ INPUT CONTEXT - Agent: ${agent.id} (Iteration: ${agent.iteration})`);
  console.log(`${'='.repeat(80)}`);
  console.log(context);
  console.log(`${'='.repeat(80)}\n`);
}

async function applyValidatorJitter(agent) {
  // LOCK CONTENTION FIX: Add random jitter for validators to prevent thundering herd
  // When multiple validators wake on the same trigger (e.g., IMPLEMENTATION_READY),
  // they all try to spawn Claude CLI at the same time. Claude CLI uses a lock file
  // per workspace, so only one can run. Adding jitter staggers their starts.
  // SKIP in testMode - tests use mocks and don't need jitter
  if (agent.role !== 'validator' || agent.testMode) {
    return;
  }

  const jitterMs = Math.floor(Math.random() * 15000); // 0-15 seconds
  if (!agent.quiet) {
    agent._log(
      `[Agent ${agent.id}] Adding ${Math.round(jitterMs / 1000)}s jitter to prevent lock contention`
    );
  }
  await new Promise((resolve) => setTimeout(resolve, jitterMs));
}

function publishTaskStarted(agent, triggeringMessage) {
  const modelSpec = agent._resolveModelSpec ? agent._resolveModelSpec() : null;
  agent._publishLifecycle('TASK_STARTED', {
    iteration: agent.iteration,
    model: agent._selectModel(),
    provider: agent._resolveProvider ? agent._resolveProvider() : 'claude',
    modelSpec,
    triggeredBy: triggeringMessage.topic,
    triggerFrom: triggeringMessage.sender,
  });
}

function attachResultMetadata(agent, result) {
  // Add task ID to result for debugging and hooks
  result.taskId = agent.currentTaskId;
  result.agentId = agent.id;
  result.iteration = agent.iteration;
}

function assertTaskSuccess(result) {
  if (!result.success) {
    throw new Error(result.error || 'Task execution failed');
  }
}

async function executeProviderTask({ agent, triggeringMessage, context }) {
  // Spawn provider task
  agent.state = 'executing_task';
  await applyValidatorJitter(agent);
  publishTaskStarted(agent, triggeringMessage);

  const result = await agent._spawnClaudeTask(context);
  attachResultMetadata(agent, result);
  assertTaskSuccess(result);
  return result;
}

function publishTaskCompleted(agent, result) {
  // Set state to idle BEFORE publishing lifecycle event
  // (so lifecycle message includes correct state)
  agent.state = 'idle';

  // Track completion time for context filtering (used by "since: last_task_end")
  agent.lastTaskEndTime = Date.now();

  agent._publishLifecycle('TASK_COMPLETED', {
    iteration: agent.iteration,
    success: true,
    taskId: agent.currentTaskId,
    tokenUsage: result.tokenUsage || null,
  });
}

function publishTokenUsage(agent, result) {
  if (!result.tokenUsage) {
    return;
  }

  // Get actual model used from API response (more accurate than config)
  const actualModel = result.tokenUsage.modelUsage
    ? Object.keys(result.tokenUsage.modelUsage)[0]
    : agent._selectModel();

  agent.messageBus.publish({
    cluster_id: agent.cluster.id,
    topic: 'TOKEN_USAGE',
    sender: agent.id,
    content: {
      text: `${agent.id} used ${result.tokenUsage.inputTokens} input + ${result.tokenUsage.outputTokens} output tokens (${actualModel})`,
      data: {
        agentId: agent.id,
        role: agent.role,
        model: agent._selectModel(),
        iteration: agent.iteration,
        taskId: agent.currentTaskId, // Causal linking for message ordering
        ...result.tokenUsage,
      },
    },
  });
}

async function runOnCompleteHookWithRetry({ agent, triggeringMessage, result }) {
  const hookMaxRetries = 3;
  const hookBaseDelay = 1000;

  for (let hookAttempt = 1; hookAttempt <= hookMaxRetries; hookAttempt++) {
    try {
      await executeHook({
        hook: agent.config.hooks?.onComplete,
        agent: agent,
        message: triggeringMessage,
        result: result,
        messageBus: agent.messageBus,
        cluster: agent.cluster,
        orchestrator: agent.orchestrator,
      });
      return;
    } catch (hookError) {
      console.error(`\n${'='.repeat(80)}`);
      console.error(
        `ðŸ”´ HOOK EXECUTION FAILED - AGENT: ${agent.id} (Attempt ${hookAttempt}/${hookMaxRetries})`
      );
      console.error(`${'='.repeat(80)}`);
      console.error(`Error: ${hookError.message}`);

      if (hookAttempt < hookMaxRetries) {
        const delay = hookBaseDelay * Math.pow(2, hookAttempt - 1);
        console.error(`Will retry hook in ${delay}ms...`);
        console.error(`${'='.repeat(80)}\n`);
        await new Promise((resolve) => setTimeout(resolve, delay));
        continue;
      }

      console.error(`${'='.repeat(80)}\n`);
      throw new Error(
        `Hook execution failed after ${hookMaxRetries} attempts. ` +
          `Task completed successfully but hook could not publish result. ` +
          `Original error: ${hookError.message}`
      );
    }
  }
}

async function handleTaskSuccess({ agent, triggeringMessage, result }) {
  publishTaskCompleted(agent, result);
  publishTokenUsage(agent, result);
  await runOnCompleteHookWithRetry({ agent, triggeringMessage, result });
}

function isLockFileError(error) {
  return Boolean(error?.message && error.message.includes('Lock file'));
}

function logTaskAttemptFailure({ agent, attempt, maxRetries, error }) {
  console.error(`\n${'='.repeat(80)}`);
  console.error(`ðŸ”´ TASK EXECUTION FAILED - AGENT: ${agent.id} (Attempt ${attempt}/${maxRetries})`);
  console.error(`${'='.repeat(80)}`);
  console.error(`Error: ${error.message}`);
}

async function applyLockContentionDelay() {
  const lockDelay = 10000 + Math.floor(Math.random() * 20000); // 10-30 seconds
  console.error(
    `âš ï¸ Lock contention detected - waiting ${Math.round(lockDelay / 1000)}s before retry`
  );
  await new Promise((resolve) => setTimeout(resolve, lockDelay));
}

function logRetryDelay(baseDelay, attempt) {
  console.error(`Will retry in ${baseDelay * Math.pow(2, attempt - 1)}ms...`);
}

function logTaskFailureFooter() {
  console.error(`${'='.repeat(80)}\n`);
}

function logMaxRetriesExhausted({ agent, maxRetries, error }) {
  console.error(`\n${'='.repeat(80)}`);
  console.error(`ðŸ”´ðŸ”´ðŸ”´ MAX RETRIES EXHAUSTED - AGENT: ${agent.id} ðŸ”´ðŸ”´ðŸ”´`);
  console.error(`${'='.repeat(80)}`);
  console.error(`All ${maxRetries} attempts failed`);
  console.error(`Final error: ${error.message}`);
  console.error(`Stack: ${error.stack}`);
  console.error(`${'='.repeat(80)}\n`);
}

function publishValidatorCrashRejection({ agent, error, maxRetries }) {
  console.error(`\n${'='.repeat(80)}`);
  console.error(`âŒ VALIDATOR CRASHED - REJECTING (NOT AUTO-APPROVING)`);
  console.error(`${'='.repeat(80)}`);
  console.error(`Validator ${agent.id} crashed ${maxRetries} times`);
  console.error(`Error: ${error.message}`);
  console.error(`REJECTING validation - broken code will NOT be merged`);
  console.error(`Investigation required before retry`);
  console.error(`${'='.repeat(80)}\n`);

  // Publish REJECTION message (NOT approval!)
  const hook = agent.config.hooks?.onComplete;
  if (hook && hook.action === 'publish_message') {
    agent._publish({
      topic: hook.config.topic,
      receiver: hook.config.receiver || 'broadcast',
      content: {
        text: `REJECTED: Validator crashed ${maxRetries} times - ${error.message}`,
        data: {
          approved: false, // REJECT!
          crashedAfterRetries: true,
          errors: JSON.stringify([
            `VALIDATOR CRASHED ${maxRetries}x: ${error.message}`,
            `Validation could not be performed - REJECTING to prevent broken code merge`,
            `Investigation required before retry`,
          ]),
          attempts: maxRetries,
          requiresInvestigation: true,
        },
      },
    });
  }
}

function setClusterFailureInfo({ agent, error, maxRetries }) {
  // Save failure info to cluster for resume capability
  agent.cluster.failureInfo = {
    agentId: agent.id,
    taskId: agent.currentTaskId,
    iteration: agent.iteration,
    error: error.message,
    attempts: maxRetries,
    timestamp: Date.now(),
  };
}

function publishAgentError({ agent, triggeringMessage, error, maxRetries }) {
  // Publish error to message bus for visibility in logs
  agent._publish({
    topic: 'AGENT_ERROR',
    receiver: 'broadcast',
    content: {
      text: `Task execution failed after ${maxRetries} attempts: ${error.message}`,
      data: {
        error: error.message,
        stack: error.stack,
        agent: agent.id,
        role: agent.role,
        iteration: agent.iteration,
        taskId: agent.currentTaskId,
        attempts: maxRetries,
        hookFailureContext: error.message.includes('Hook uses result')
          ? {
              taskId: agent.currentTaskId || 'UNKNOWN',
              retrieveLogs: agent.currentTaskId
                ? `zeroshot task logs ${agent.currentTaskId}`
                : 'N/A',
            }
          : undefined,
      },
    },
    metadata: {
      triggeringTopic: triggeringMessage.topic,
    },
  });
}

async function runOnErrorHook({ agent, triggeringMessage, error }) {
  await executeHook({
    hook: agent.config.hooks?.onError,
    agent: agent,
    message: triggeringMessage,
    result: { error },
    messageBus: agent.messageBus,
    cluster: agent.cluster,
    orchestrator: agent.orchestrator,
  });
}

async function handleExhaustedRetries({ agent, triggeringMessage, error, maxRetries }) {
  logMaxRetriesExhausted({ agent, maxRetries, error });

  // CRITICAL FIX: Validator crash = REJECTION (not auto-approval)
  // Auto-approval on crash allowed broken code to be merged - unacceptable!
  // If validator crashed 3x, something is fundamentally wrong - REJECT and investigate
  if (agent.role === 'validator') {
    publishValidatorCrashRejection({ agent, error, maxRetries });
  }

  agent.state = 'error';
  setClusterFailureInfo({ agent, error, maxRetries });
  publishAgentError({ agent, triggeringMessage, error, maxRetries });
  await runOnErrorHook({ agent, triggeringMessage, error });
  agent.state = 'idle';
}

async function scheduleRetry({ agent, attempt, maxRetries, baseDelay, error }) {
  const delay = baseDelay * Math.pow(2, attempt - 1); // 2s, 4s, 8s

  agent._publishLifecycle('RETRY_SCHEDULED', {
    attempt,
    maxRetries,
    delayMs: delay,
    error: error.message,
  });

  agent._log(`[${agent.id}] âš ï¸  Retrying in ${delay}ms... (${attempt + 1}/${maxRetries})`);

  // Exponential backoff
  await new Promise((resolve) => setTimeout(resolve, delay));

  agent._log(`[${agent.id}] ðŸ”„ Starting retry attempt ${attempt + 1}/${maxRetries}`);
}

async function handleTaskFailure({
  agent,
  triggeringMessage,
  attempt,
  maxRetries,
  baseDelay,
  error,
}) {
  const lockError = isLockFileError(error);
  logTaskAttemptFailure({ agent, attempt, maxRetries, error });

  if (lockError) {
    // Lock contention - add significant jittered delay
    await applyLockContentionDelay();
  } else if (attempt < maxRetries) {
    logRetryDelay(baseDelay, attempt);
  }
  logTaskFailureFooter();

  if (attempt >= maxRetries) {
    await handleExhaustedRetries({ agent, triggeringMessage, error, maxRetries });
    return false;
  }

  await scheduleRetry({ agent, attempt, maxRetries, baseDelay, error });
  return true;
}

/**
 * Execute claude-zeroshots with built context
 * Retries disabled by default. Set agent config `maxRetries` to enable (e.g., 3).
 * @param {AgentWrapper} agent - Agent instance
 * @param {Object} triggeringMessage - Message that triggered execution
 */
async function executeTask(agent, triggeringMessage) {
  // Early exit if agent was stopped
  if (!agent.running) {
    return;
  }

  // Default: no retries (maxRetries=1 means 1 attempt only)
  // Set agent config `maxRetries: 3` to enable exponential backoff retries
  const maxRetries = agent.config.maxRetries ?? 1;
  const baseDelay = 2000; // 2 seconds

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    // Check if agent was stopped between retries
    if (!agent.running) {
      return;
    }

    try {
      await runOnStartHook({ agent, triggeringMessage });

      // Check max iterations limit BEFORE incrementing (prevents infinite rejection loops)
      if (hasReachedMaxIterations(agent)) {
        handleMaxIterationsReached(agent);
        return;
      }

      // Increment iteration BEFORE building context so worker knows current iteration
      agent.iteration++;

      const context = buildTaskContext({ agent, triggeringMessage });
      logInputContext(agent, context);

      const result = await executeProviderTask({ agent, triggeringMessage, context });
      await handleTaskSuccess({ agent, triggeringMessage, result });
      return;
    } catch (error) {
      const shouldRetry = await handleTaskFailure({
        agent,
        triggeringMessage,
        attempt,
        maxRetries,
        baseDelay,
        error,
      });
      if (!shouldRetry) {
        return;
      }
    }
  }
}

/**
 * Start monitoring agent output liveness using multi-indicator stuck detection
 *
 * SAFE DETECTION: Only flags as stuck when MULTIPLE indicators agree:
 * - Process sleeping (state=S)
 * - Blocked on epoll/poll wait
 * - Low CPU usage (<1%)
 * - Low context switches (<10)
 * - No network data in flight
 *
 * Single-indicator detection (just output freshness) has HIGH false positive risk.
 * This multi-indicator approach eliminates false positives.
 *
 * @param {AgentWrapper} agent - Agent instance
 */
function startLivenessCheck(agent) {
  if (agent.livenessCheckInterval) {
    clearInterval(agent.livenessCheckInterval);
  }

  // Check if platform supports /proc filesystem (Linux only)
  if (!isPlatformSupported()) {
    agent._log(
      `[${agent.id}] Liveness check disabled: /proc filesystem not available (non-Linux platform)`
    );
    return;
  }

  // Check every 60 seconds (gives time for multi-indicator analysis)
  const CHECK_INTERVAL_MS = 60 * 1000;
  const ANALYSIS_SAMPLE_MS = 5000; // Sample CPU/context switches over 5 seconds

  agent.livenessCheckInterval = setInterval(async () => {
    // Skip if no task running or no PID tracked
    if (!agent.currentTask || !agent.processPid) {
      return;
    }

    // Skip if output is recent (process is clearly active)
    if (agent.lastOutputTime) {
      const timeSinceLastOutput = Date.now() - agent.lastOutputTime;
      if (timeSinceLastOutput < agent.staleDuration) {
        return; // Output is recent, definitely not stuck
      }
    }

    // Output is stale - run multi-indicator analysis to confirm
    agent._log(
      `[${agent.id}] Output stale for ${Math.round((Date.now() - (agent.lastOutputTime || 0)) / 1000)}s, running multi-indicator analysis...`
    );

    try {
      const analysis = await analyzeProcessHealth(agent.processPid, ANALYSIS_SAMPLE_MS);

      // Process died during analysis
      if (analysis.isLikelyStuck === null) {
        agent._log(`[${agent.id}] Process analysis inconclusive: ${analysis.reason}`);
        return;
      }

      // Log analysis details for debugging
      agent._log(
        `[${agent.id}] Analysis: score=${analysis.stuckScore}/${STUCK_THRESHOLD}, ` +
          `state=${analysis.state}, wchan=${analysis.wchan}, ` +
          `CPU=${analysis.cpuPercent}%, ctxSwitches=${analysis.ctxSwitchesDelta}`
      );

      if (analysis.isLikelyStuck) {
        agent._log(`âš ï¸  Agent ${agent.id}: CONFIRMED STUCK (confidence: ${analysis.confidence})`);
        agent._log(`    ${analysis.analysis}`);

        // CHANGED: Stale detection is informational only - never kills tasks
        // Publish stale detection event with full analysis (for logging/monitoring)
        agent._publishLifecycle('AGENT_STALE_WARNING', {
          timeSinceLastOutput: Date.now() - (agent.lastOutputTime || 0),
          staleDuration: agent.staleDuration,
          lastOutputTime: agent.lastOutputTime,
          // Multi-indicator analysis results
          stuckScore: analysis.stuckScore,
          confidence: analysis.confidence,
          processState: analysis.state,
          wchan: analysis.wchan,
          cpuPercent: analysis.cpuPercent,
          ctxSwitchesDelta: analysis.ctxSwitchesDelta,
          indicators: analysis.indicators,
          analysis: analysis.analysis,
        });

        // Keep monitoring - do NOT stop the agent
        // User can manually intervene with 'zeroshot resume' if needed
        // stopLivenessCheck(agent); // REMOVED - keep monitoring
      } else {
        agent._log(
          `[${agent.id}] Process appears WORKING despite stale output (score: ${analysis.stuckScore})`
        );
        agent._log(`    ${analysis.analysis}`);
        // Don't flag as stuck - process is legitimately working
      }
    } catch (err) {
      agent._log(`[${agent.id}] Error during stuck analysis: ${err.message}`);
      // Don't flag as stuck on analysis error
    }
  }, CHECK_INTERVAL_MS);
}

/**
 * Stop liveness monitoring
 * @param {AgentWrapper} agent - Agent instance
 */
function stopLivenessCheck(agent) {
  if (agent.livenessCheckInterval) {
    clearInterval(agent.livenessCheckInterval);
    agent.livenessCheckInterval = null;
  }
}

module.exports = {
  start,
  stop,
  handleMessage,
  executeTriggerAction,
  executeTask,
  startLivenessCheck,
  stopLivenessCheck,
};
